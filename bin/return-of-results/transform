#!/usr/bin/env python3
"""
Join the REDCap NDJSON data with an ID3C CSV.
Parses the joined data to make it suitable for UW Lab Med's return of results
portal.
"""
import sys
import argparse
import pandas as pd


def parse_scan_redcap(redcap_file) -> pd.DataFrame:
    """
    Reads in data from a given SCAN *redcap_file*. Returns a pandas.DataFrame
    prepared in the specifications of UW Lab Med's return of results portal.
    """
    redcap_data = (
        pd.read_json(redcap_file, lines = True, dtype = False, convert_dates = False)
        .astype("string")
        .pipe(trim_whitespace)
        .replace({"": pd.NA})
        .astype("string")
        .rename(columns={'birthday': 'birth_date'}))

    participant_name = lambda row: f"{row['participant_first_name']} {row['participant_last_name']}"
    redcap_data['pat_name'] = redcap_data.apply(participant_name, axis='columns')

    # This invariant protects our filename assumptions.
    assert all(redcap_data['birth_date'].str.match(r"^\d{4}-\d{2}-\d{2}$").dropna())

    # Normalize all barcode fields upfront.
    barcode_fields = {
        "return_utm_barcode",
        "utm_tube_barcode_2",
        "reenter_barcode",
        "reenter_barcode_2"}

    for barcode_field in barcode_fields:
        if barcode_field in redcap_data:
            redcap_data[barcode_field] = normalize_barcode(redcap_data[barcode_field])
        else:
            redcap_data[barcode_field] = pd.Series(dtype = "string")

    # For our at-home mailed kits, return_utm_barcode is the most reliable.  It
    # is scanned during unboxing of returned kits after confirming the
    # participant's identity.  At-home kits also have a utm_tube_barcode_2
    # which is manually entered by the participant; it is double-entered into
    # reenter_barcode.  We *do not* use participant-entered barcodes for mailed
    # kits, so those get pre-emptively blanked.
    #
    # For our kiosk/in-person collections, utm_tube_barcode_2 is scanned by
    # staff.  If scanning doesn't work, then a manually-entered barcode is in
    # reenter_barcode; it is double-entered into reenter_barcode_2.
    collection_via = redcap_data["project_collection_via"]

    redcap_data.loc[collection_via != "kiosk", "utm_tube_barcode_2"] = pd.NA
    redcap_data.loc[collection_via != "kiosk", "reenter_barcode"]    = pd.NA

    return_utm_barcode = redcap_data["return_utm_barcode"]
    utm_tube_barcode_2 = redcap_data["utm_tube_barcode_2"]
    reenter_barcode    = redcap_data["reenter_barcode"]
    reenter_barcode_2  = redcap_data["reenter_barcode_2"]

    # Censor manually-entered barcodes which don't match.
    mismatched = (
          (reenter_barcode.notna() | reenter_barcode_2.notna())
        & reenter_barcode.ne(reenter_barcode_2, fill_value = ""))

    if not mismatched[mismatched].empty:
        print(f"Censoring {mismatched[mismatched].size:,} manually-entered barcodes which do not match each other", file = sys.stderr)

    redcap_data.loc[mismatched, "reenter_barcode"]   = pd.NA
    redcap_data.loc[mismatched, "reenter_barcode_2"] = pd.NA

    assert all(reenter_barcode.eq(reenter_barcode_2, fill_value = "").dropna())

    # Use best barcode available depending on the project.
    #
    # Note that combine_first() only applies for NA values (e.g. not empty
    # strings); the NA-handling above is important.
    mail_barcodes  = return_utm_barcode
    kiosk_barcodes = utm_tube_barcode_2.combine_first(reenter_barcode)

    barcodes = kiosk_barcodes.where(collection_via == "kiosk", mail_barcodes)

    # There should be absolutely no duplicates, but REDCap can't enforce this,
    # so warn and drop any that we find.
    barcodes = drop_duplicate_barcodes(barcodes)

    redcap_data['qrcode'] = barcodes
    redcap_data['source'] = 'scan'

    return redcap_data[['qrcode', 'pat_name', 'birth_date', 'contacted', 'source']]


def parse_sfs_longitudinal_redcap(redcap_file) -> pd.DataFrame:
    """
    Reads in data from a given SFS longitudinal *redcap_file*. Returns a
    pandas.DataFrame prepared in the specifications of UW Lab Med's return
    of results portal.
    """
    redcap_data = (
        pd.read_json(redcap_file, lines = True, dtype = False, convert_dates = False)
        .astype("string")
        .pipe(trim_whitespace)
        .replace({"": pd.NA})
        .astype("string")
        .rename(columns={"core_birthdate": "birth_date"})
    )

    participant_name = lambda row: f"{row['core_participant_first_name']} {row['core_participant_last_name']}"
    redcap_data['pat_name'] = redcap_data.apply(participant_name, axis='columns')

    # This invariant protects our filename assumptions.
    assert all(redcap_data['birth_date'].str.match(r"^\d{4}-\d{2}-\d{2}$").dropna())

    # Normalize all barcode fields upfront.
    barcode_fields = {
        "collect_barcode_kiosk",
        "return_utm_barcode"}

    for barcode_field in barcode_fields:
        redcap_data[barcode_field] = normalize_barcode(redcap_data[barcode_field])

    kiosk_barcodes = redcap_data["collect_barcode_kiosk"]
    mail_barcodes = redcap_data["return_utm_barcode"]

    barcodes = kiosk_barcodes.combine_first(mail_barcodes)

    barcodes = drop_duplicate_barcodes(barcodes)

    redcap_data['qrcode'] = barcodes
    redcap_data.loc[redcap_data['source'] == 'uw_reopening', 'contacted'] = pd.NA

    return redcap_data[['qrcode', 'pat_name', 'birth_date', 'contacted', 'source']]


def parse_sfs_classic_redcap(redcap_file) -> pd.DataFrame:
    """
    Reads in data from a given SFS classic *redcap_file*. Returns a
    pandas.DataFrame prepared in the specifications of UW Lab Med's return
    of results portal.
    """
    redcap_data = (
        pd.read_json(redcap_file, lines = True, dtype = False, convert_dates = False)
        .astype("string")
        .pipe(trim_whitespace)
        .replace({"": pd.NA})
        .astype("string")
        .rename(columns={"core_participant_birth_date": "birth_date", "participant_contacted": "contacted"})
    )

    participant_name = lambda row: f"{row['core_participant_first_name']} {row['core_participant_last_name']}"
    redcap_data['pat_name'] = redcap_data.apply(participant_name, axis='columns')

    # This invariant protects our filename assumptions.
    assert all(redcap_data['birth_date'].str.match(r"^\d{4}-\d{2}-\d{2}$").dropna())

    redcap_data['prioritized_barcode'] = normalize_barcode(redcap_data['prioritized_barcode'])
    barcodes = redcap_data['prioritized_barcode']
    barcodes = drop_duplicate_barcodes(barcodes)
    redcap_data['qrcode'] = barcodes

    return redcap_data[['qrcode', 'pat_name', 'birth_date', 'contacted', 'source', 'contact_before_releasing_result', 'ordering_provider']]


def normalize_barcode(barcode):
    return barcode.str.upper().str.strip()


def trim_whitespace(df: pd.DataFrame) -> pd.DataFrame:
    """
    Trim leading and trailing whitespace from strings in *df*.
    """
    str_columns = df.select_dtypes("string").columns
    df[str_columns] = df[str_columns].apply(lambda column: column.str.strip())
    return df


def drop_duplicate_barcodes(barcodes: pd.Series) -> pd.Series:
    """
    Find and drop duplicate barcodes from the provided *barcodes* and returns
    the deduplicated pandas Series
    """
    dups = barcodes.loc[barcodes.duplicated(keep = False)].dropna()

    if not dups.empty:
        barcodes = barcodes.drop(dups.index, inplace = False)

        dups_count = len(dups)
        dups_unique = list(dups.unique())
        print(f"Dropped {dups_count:,} REDCap records with duplicated barcodes: {dups_unique}", file = sys.stderr)

    return barcodes


def edit_status_code(results: pd.DataFrame) -> pd.DataFrame:
    """
    Sets the *results* status to `pending` if a test result is `positive` or
    `inconclusive` but the participant has not yet been contacted
    (`contacted` != 'yes') for SCAN and SFS Childcare samples or for records
    with `contact_before_releasing_result` == 'true'

    Sets the *results* status to `pending` if a record has a null swab type.

    Returns the modified *results*.
    """
    # Assume a null value for `contacted` means not contacted
    results.loc[results['contacted'].isnull(), 'contacted'] = 'no'

    is_positive = results['status_code'] == 'positive'
    is_inconclusive = results['status_code'] == 'inconclusive'

    participant_contacted = results['contacted'] == 'yes'

    # To prevent a row's `require_contacted` value from getting set as pd.NA and then failing where we compare
    # its value with the boolean values of the other series (is_positive, is_inconclusive, participant_contacted),
    # first check that the value is not pd.NA before comparing its string value.
    require_contacted_lambda = lambda row: (row['source'] is not pd.NA and row['source'] \
        in ['scan', 'childcare', 'snohomish_schools', '2021_childcare', 'apple_respiratory']) \
        or (row['contact_before_releasing_result'] is not pd.NA and row['contact_before_releasing_result'] == 'true')
    require_contacted = results.apply(require_contacted_lambda, axis='columns')

    results.loc[(is_positive | is_inconclusive) & ~(participant_contacted) & require_contacted, 'status_code'] = 'pending'

    results.loc[results['swab_type'].isnull(), 'status_code'] = 'pending'

    return results


def check_data_quality(joined_data: pd.DataFrame) -> None:
    """
    Performs some data quality checks on the final `joined_data`
    DataFrame and warns (by writing to sys.stderr) about records
    that have quality issues.
    """

    # Check the `pre_analytical_specimen_collection` column
    # pd.NA is included by ~ isin
    no_pre_analytical_specimen_collection = joined_data[~joined_data['pre_analytical_specimen_collection'].isin \
        (['IRB', 'clinical'])]

    if not no_pre_analytical_specimen_collection.empty:
        print(f"{len(no_pre_analytical_specimen_collection)} records have a bad pre_analytical_specimen_collection value.",
            file = sys.stderr)
        no_pre_analytical_specimen_collection.apply \
            (lambda row: print(f"qrcode: «{row['qrcode']}», pre_analytical_specimen_collection: "
            f"«{row['pre_analytical_specimen_collection']}»",
                file = sys.stderr), axis='columns')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Join a REDCAP export NDJSON file with a SCAN return of results ID3C export CSV file."
    )
    parser.add_argument("scan_redcap_data", help="NDJSON export of SCAN records from REDCap")
    parser.add_argument("sfs_longitudinal_redcap_data", help="NDJSON export of SFS longitudinal records from REDCap")
    parser.add_argument("sfs_classic_redcap_data", help="NDJSON export of SFS classic records from REDCap")
    parser.add_argument("id3c_data", help="CSV export of SCAN return of results from ID3C")
    parser.add_argument("output", help="A destination for the output csv", nargs="?",
        default=sys.stdout)

    args = parser.parse_args()

    scan_redcap_data = parse_scan_redcap(args.scan_redcap_data)
    sfs_longitudinal_redcap_data = parse_sfs_longitudinal_redcap(args.sfs_longitudinal_redcap_data)
    sfs_classic_redcap_data = parse_sfs_classic_redcap(args.sfs_classic_redcap_data)

    # Combine the DataFrames by appending to the first.
    # Use `ignore_index = True` to prevent duplicate index values.
    redcap_data = scan_redcap_data.append([sfs_longitudinal_redcap_data, sfs_classic_redcap_data], ignore_index = True)

    # Check for duplicate barcodes across ALL projects.
    redcap_data["qrcode"] = drop_duplicate_barcodes(redcap_data["qrcode"])

    id3c_data = pd.read_csv(args.id3c_data, dtype = 'string')
    id3c_data['qrcode'] = normalize_barcode(id3c_data['qrcode'])

    # Only keep records we can match from both REDCap and ID3C.  An incorrect
    # barcode in REDCap or the lack of a record in ID3C may cause records to
    # drop out in the join, but this is preferable to attaching results to an
    # incorrect barcode or not having a known status for a barcode.
    joined_data = redcap_data.merge(id3c_data, how='inner', on='qrcode')
    joined_data = edit_status_code(joined_data)

    check_data_quality(joined_data)

    # Caution: a side effect of adding columns here is that the pipeline will consider all results as new
    # when it diffs against the previous run's output, and attempt to regenerate all results PDFs.
    # That process is incredibly slow, so it may be prudent to manually backfill new columns into the
    # previous_results.csv in S3

    joined_data[['qrcode', 'pat_name', 'birth_date', 'collect_ts', 'result_ts', 'status_code', 'swab_type', 'staff_observed', 'pre_analytical_specimen_collection', 'ordering_provider']].to_csv(args.output, index=False)
