#!/usr/bin/env python3
"""
Join the REDCap NDJSON data with an ID3C CSV.
Prepares the joined data for import into the
EH&S Transfer REDCap project.
"""

import argparse
import io
import json
import pandas as pd
import sys
from typing import List

from id3c.cli.io.pandas import dump_ndjson

ENROLL_FIELDS =  [
        "netid",
        "affiliation_other",
        "uw_school",
        "employee_category_other",
        "greek_other",
        "participant_first_name",
        "participant_last_name",
        "birthdate",
        "uw_dorm_room_number",
        "phone_number",
        "phone_number_2",
        "email",
        "home_street",
        "apartment_number",
        "home_city",
        "home_state",
        "zipcode",
        "preferred_pronouns_other",
        "sex_other",
        "uw_greek_house",
        "uw_dorm_name",
        "uw_apartment_name"]

# The number of lines to read from the REDCap file in one batch
BATCH_SIZE = 10000


def prepare_id3c_data(id3c_data_path: str) -> pd.DataFrame:
    """
    Reads the file containing results data from ID3C.
    Prepares the data for joining to REDCap data later.
    """
    id3c_data = pd.read_csv(id3c_data_path, dtype = 'string')
    id3c_data['barcode'] = normalize_barcode(id3c_data['barcode'])

    # Drop id3c records with duplicated barcodes
    barcodes = id3c_data['barcode']
    barcodes = drop_duplicate_values(barcodes)
    id3c_data['barcode'] = barcodes
    id3c_data.dropna(subset={'barcode'}, inplace=True)

    # When you export a record set from Postgres as CSV, boolean columns
    # get values of 't' and 'f'. The EH&S transfer REDCap project expects
    # values of 1 and 0.
    boolean_fields = [
        'is_student_athlete',
        'is_taking_inperson_classes',
        'works_at_uw',
        'is_uw_greek_member',
        'lives_with_uw_students_or_employees',
        'lives_in_uw_apartment'
    ]

    for field in boolean_fields:
        id3c_data[field].replace({'t': 1, 'f': 0}, inplace=True)

    return id3c_data


def find_duplicate_netids(redcap_file: str) -> List[str]:
    """
    From the *redcap_file* finds all NetIDs that are associated with
    more than one record_id.
    """
    found_netids = {}
    duplicates = []

    with open(redcap_file, 'r') as f:
        for line in f:
            line_json = json.loads(line)
            if line_json['netid']: # Only enrollment records have netid populated

                if line_json['netid'] in found_netids:

                    if found_netids[line_json['netid']] != line_json['record_id']:
                        duplicates.append(line_json['netid'])

                else:
                    found_netids[line_json['netid']] = line_json['record_id']

    return duplicates


def process_redcap_data_and_dump_output(redcap_file: str, id3c_data: pd.DataFrame,
        output: argparse.FileType) -> None:
    """
    Reads the file containing REDCap data in batches.
    Joins each REDCap batch with ID3C data and writes the joined
    data as ndjson.
    """
    duplicate_netids = find_duplicate_netids(redcap_file)
    if len(duplicate_netids) > 0:
        # Don't log the actual NetIDs to protect privacy.
        print(f"Found {len(duplicate_netids)} NetIDs that belong to more than one record_id. "
            f"These will be dropped from enrollments.", file = sys.stderr)

    with open(redcap_file, "r") as file:
        rows = []

        for row in (json.loads(line) for line in file):
            if len(rows) >= BATCH_SIZE and row["record_id"] != rows[-1]["record_id"]:
                # We've made our batch size and are at a record boundary.  Process the
                # current batch now.
                process_redcap_batch_and_dump_output(rows, id3c_data, duplicate_netids, output)

                # Start the next batch with the current row because it has not been processed yet.
                rows = [row]

            else:
                rows.append(row)

        # Pick up the last batch
        if rows:
            process_redcap_batch_and_dump_output(rows, id3c_data, duplicate_netids, output)


def process_redcap_batch_and_dump_output(rows: List[dict], id3c_data: pd.DataFrame,
        duplicate_netids: List[str], output: argparse.FileType) -> None:

        redcap_data = (
            pd.DataFrame.from_records(rows)
            .astype("string")
            .pipe(trim_whitespace)
            .replace({"": pd.NA})
            .astype("string")
            .rename(columns={"dorm_room": "uw_dorm_room_number",
            "sea_employee_type_other": "employee_category_other",
            "dorm": "uw_dorm_name",
            "uw_apt_names": "uw_apartment_name",
            "core_participant_first_name": "participant_first_name",
            "core_participant_last_name": "participant_last_name",
            "core_birthdate": "birthdate",
            "core_home_street": "home_street",
            "core_apartment_number": "apartment_number",
            "core_home_city": "home_city",
            "core_home_state": "home_state",
            "core_zipcode": "zipcode",
            "pronouns_other": "preferred_pronouns_other",
            "core_sex_other": "sex_other"
            }))

        # Normalize all barcode fields upfront.
        barcode_fields = {
            "collect_barcode_kiosk",
            "return_utm_barcode"}

        for barcode_field in barcode_fields:
            if barcode_field in redcap_data:
                redcap_data[barcode_field] = normalize_barcode(redcap_data[barcode_field])
            else:
                redcap_data[barcode_field] = pd.Series(dtype = "string")

        kiosk_barcodes = redcap_data["collect_barcode_kiosk"]
        mail_barcodes = redcap_data["return_utm_barcode"]

        barcodes = kiosk_barcodes.combine_first(mail_barcodes)
        barcodes = drop_duplicate_values(barcodes)
        redcap_data['barcode'] = barcodes

        enrollments = redcap_data.query('redcap_event_name == "enrollment_arm_1"')[[ \
            "record_id",
            "netid",
            "affiliation_other",
            "uw_school",
            "employee_category_other",
            "greek_other",
            "participant_first_name",
            "participant_last_name",
            "birthdate",
            "uw_dorm_room_number",
            "phone_number",
            "phone_number_2",
            "email",
            "home_street",
            "apartment_number",
            "home_city",
            "home_state",
            "zipcode",
            "preferred_pronouns_other",
            "sex_other",
            "uw_greek_house",
            "uw_dorm_name",
            "uw_apartment_name"
            ]]

        # This invariant protects our filename assumptions.
        assert all(enrollments['birthdate'].str.match(r"^\d{4}-\d{2}-\d{2}$").dropna())

        # Drop enrollment records where the NetID was duplicated across record_ids.
        enrollments = enrollments[~enrollments['netid'].isin(duplicate_netids)]

        encounters = redcap_data.query('redcap_event_name == "encounter_arm_1"')[[ \
            'record_id',
            'barcode',
            'illness_kiosk',
            'illness_swabsend'
            ]]

        # Set symptomatic_when_tested on encounters.
        # Use '1' and '0' values to be consistent with the other boolean values we send to REDCap.
        encounters.loc[(encounters['illness_kiosk'] == 'yes') | (encounters['illness_swabsend'] == 'yes'), 'symptomatic_when_tested'] = '1'
        encounters.loc[(encounters['illness_kiosk'] == 'no') | (encounters['illness_swabsend'] == 'no'), 'symptomatic_when_tested'] = '0'

        encounters = enrollments.merge(encounters, how='inner', on='record_id')

        # Only keep records we can match from both REDCap and ID3C.  An incorrect
        # barcode in REDCap or the lack of a record in ID3C may cause records to
        # drop out in the join, but this is preferable to attaching results to an
        # incorrect barcode or not having a known status for a barcode.
        encounters = encounters.merge(id3c_data, how='inner', on='barcode')

        dump_joined_data(encounters, output)


def dump_joined_data(joined_data: pd.DataFrame, output: argparse.FileType) -> None:
    """
    Writes the *joined_data* DataFrame as ndjson to the *output* file.
    """
    dump_ndjson(joined_data[[ \
        'barcode', 'sample_collection_date', 'test_result', 'test_result_date',

        'netid', 'participant_first_name', 'participant_last_name', 'birthdate',
        'phone_number', 'phone_number_2', 'email' , 'home_street',
        'apartment_number', 'home_city', 'home_state', 'zipcode',

        'preferred_pronouns', 'preferred_pronouns_other', 'sex', 'sex_other',
        'age_at_encounter', 'preferred_contact_method_for_study',
        'preferred_contact_method_for_attestations', 'study_enrollment_date_time',
        'campus_location', 'affiliation', 'affiliation_other', 'uw_school',

        'is_student_athlete', 'student_level', 'employee_category', 'employee_category_other',
        'is_taking_inperson_classes', 'works_at_uw', 'on_campus_frequency_code',
        'on_campus_frequency_description', 'able_to_work_or_study_from_home_code',
        'able_to_work_or_study_from_home_description','is_uw_greek_member', 'uw_greek_house',
        'greek_other', 'housing_type', 'number_house_members', 'lives_with_uw_students_or_employees',
        'uw_dorm_name', 'uw_dorm_room_number', 'lives_in_uw_apartment',
        'uw_apartment_name', 'study_tier', 'symptomatic_when_tested'

        ]], file = output)


def normalize_barcode(barcode):
    if barcode.empty:
        return pd.NA
    return barcode.str.upper().str.strip()


def trim_whitespace(df: pd.DataFrame) -> pd.DataFrame:
    """
    Trim leading and trailing whitespace from strings in *df*.
    """
    str_columns = df.select_dtypes("string").columns
    df[str_columns] = df[str_columns].apply(lambda column: column.str.strip())
    return df


def drop_duplicate_values(input_series: pd.Series) -> pd.Series:
    """
    Find and drop duplicate values from the provided series and returns
    the deduplicated pandas Series.
    Avoids printing PII only if the given *input_series.name* is `netid`.
    """
    deduplicated_series = input_series

    dups = input_series.loc[input_series.duplicated(keep = False)].dropna()
    if not dups.empty:
        deduplicated_series = input_series.drop(dups.index, inplace = False)

        dups_count = len(dups)
        dups_unique = list(dups.unique())
        print(f"Dropped {dups_count:,} records with duplicated {input_series.name}: "
            f"{dups_unique if input_series.name != 'netid' else '<masked>'}", file = sys.stderr)

    return deduplicated_series


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Join a REDCAP export NDJSON file with a SCAN return of results ID3C export CSV file."
    )
    parser.add_argument("redcap_data", help="NDJSON export of SCAN records from REDCap")
    parser.add_argument("id3c_data", help="CSV export of SCAN return of results from ID3C")
    parser.add_argument("output", help="A destination for the output NDJSON", nargs="?",
        default=sys.stdout, type=argparse.FileType("w"))

    args = parser.parse_args()

    id3c_data = prepare_id3c_data(args.id3c_data)
    process_redcap_data_and_dump_output(args.redcap_data, id3c_data, args.output)
