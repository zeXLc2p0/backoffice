SHELL=/bin/bash
PATH=/opt/backoffice/bin:/usr/local/bin:/usr/bin:/bin

# Point Pipenv at the production environment
PIPENV_PIPFILE=/opt/backoffice/id3c-production/Pipfile

LOG_CONFIG=/opt/backoffice/id3c-production/logging.yaml
LOG_LEVEL=warning
PGSERVICE=production-etl

# Base of our env.d directories
ENVD=/opt/backoffice/id3c-production/env.d

# Configure a private runtime directory for "fatigue".
XDG_RUNTIME_DIR=/home/ubuntu/run

# Directory to hold flock files
FLOCK_DIR=/var/run/lock


# Longitudinal childcare records are manually uploaded right now, so just check
# once an hour at five past.
5 * * * * ubuntu promjob "id3c etl longitudinal" pipenv run id3c etl longitudinal --commit

# Genomes are uploaded irregularly.  Once an hour should be good for now.
10 * * * * ubuntu promjob "id3c etl consensus-genome" pipenv run id3c etl consensus-genome --commit

# Audere sends us data at quarter past.  It rarely takes more than a minute,
# but ensure we process everything they might send by waiting until 20 after.
20 * * * * ubuntu promjob "id3c etl enrollments" pipenv run id3c etl enrollments --commit

# Kit records are created/updated from enrollment records from Audere. To ensure
# that this doesn't run at the same time as the enrollments etl, run at 25 after.
# 03/03/2021: We no longer need the kit jobs to run, so commenting this out.
#25 * * * * ubuntu promjob "id3c etl kit enrollments" pipenv run id3c etl kit enrollments --commit

# Test results are pushed at arbitrary times now, so check for new ones every 15m.
*/15 * * * * ubuntu promjob "id3c etl presence-absence" pipenv run id3c etl presence-absence --commit

# 5 minutes after presence/absence results are uploaded,
# check if any contain reportable conditions.
5-50/15 * * * * ubuntu promjob "id3c reportable-conditions notify" envdir $ENVD/slack pipenv run id3c reportable-conditions notify --commit

# The aliquot manifest is uploaded to AWS at arbitrary times now, so check for new records every 10m.
# This is often the blocker for the presence/absence ETl, so run more frequently than
# the presence/absence ETL to avoid extended delay of results.
*/10 * * * * ubuntu chronic fatigue promjob "manifest update: aliquots" envdir $ENVD/aliquot-manifest-processing/ envdir $ENVD/hutch/ pipenv run /opt/specimen-manifests/update-unattended --push-and-upload

# Process the incident report manifest from Google Sheets at :25 and :55 past the hour.
# Don't run at 10 minutes on the hour so we don't interfere with the aliquot job.
25,55 * * * * ubuntu chronic fatigue promjob "manifest update: incidents" envdir $ENVD/incident-report-manifest-processing/ envdir $ENVD/google/ pipenv run /opt/specimen-manifests/update-unattended --push-and-upload

# Upload new UW retrospectives to REDCap once a day at midnight
0 0 * * * ubuntu promjob "redcap-uw-retrospectives: import-to-redcap" pipenv run chronic envdir $ENVD/hutch/ envdir $ENVD/redcap import-uw-retrospectives-to-redcap --import

# Create and upload REDCap DETs for UW retrospectives once a day at 5 A.M.
0 5 * * * ubuntu promjob "redcap-uw-retrospectives: generate-and-upload-dets" pipenv run chronic envdir $ENVD/redcap generate-and-upload-uw-retro-redcap-dets

# Run the manifest ETL 5 minutes after new manifest records are uploaded
5-55/10 * * * * ubuntu fatigue --quiet promjob "id3c etl manifest" pipenv run id3c etl manifest --commit

# Kit records are created/updated from manifest records. To ensure that this
# doesn't run at the same time as manifest etl, run ten 'till.
# 03/03/2021: We no longer need the kit jobs to run, so commenting this out.
#50 * * * * ubuntu promjob "id3c etl kit manifest" pipenv run id3c etl kit manifest --commit

# Parse and upload the latest SCH retrospective data from S3 to ID3C
51 * * * * ubuntu promjob "SCH retrospectives" chronic envdir $ENVD/hutch envdir $ENVD/deidentification upload-sch-retrospective-data-pulls --upload

# Clinical encounter records are manually uploaded right now, so just check
# once an hour at five 'till.
55 * * * * ubuntu promjob "id3c etl clinical" pipenv run id3c etl clinical --commit

# XXX TODO: Remove locking of the cache file (to prevent corruption/race
# conditions) once locking is done by ID3C directly.
#
# Ingest SFS projects every hour, interleaving the projects so each get
# ingested once every 8 hours. Keeping these in case any data corrections are made.
# The uw-retro REDCap project is still active, but it only needs to be run
# once a day after the DET generation at 5 A.M.
GEOCODING_CACHE=/home/ubuntu/sfs-cache.pickle
0 0-16/8 * * * ubuntu promjob "id3c etl redcap-det uw-retrospectives"        flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det uw-retrospectives --commit
0 1-17/8 * * * ubuntu promjob "id3c etl redcap-det swab-n-send"              flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det swab-n-send --commit
0 2-18/8 * * * ubuntu promjob "id3c etl redcap-det kiosk"                    flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det kiosk --commit
0 3-19/8 * * * ubuntu promjob "id3c etl redcap-det swab-and-home-flu"        flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det swab-and-home-flu --commit
0 5-21/8 * * * ubuntu promjob "id3c etl redcap-det asymptomatic-swab-n-send" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det asymptomatic-swab-n-send --commit

# Ingest each SCAN Surveillance REDCap project every hour, interleaving the
# projects so each gets ingested once every 8 hours.
# These projects are no longer active since we've transitioned to the
# SCAN Research Study. Keeping these in case any data corrects are made.
# Deleted jobs for Surveillance REDCap projects that did not have any records
# as of 08 June, 2020.
GEOCODING_CACHE=/home/ubuntu/scan-cache.pickle
0 5-21/8 * * * ubuntu promjob "id3c etl redcap-det scan-en"      flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-en --commit
0 6-22/8 * * * ubuntu promjob "id3c etl redcap-det scan-es"      flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-es --commit
0 7-23/8 * * * ubuntu promjob "id3c etl redcap-det scan-zh-Hant" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-zh-Hant --commit

# Ingest each SCAN Research REDCap project every 30 min (with the
# first starting 5 minutes after the SFS/SCAN Surveillance ingests above.)
5,35  * * * * ubuntu promjob "id3c etl redcap-det scan-irb-en"       flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-en --commit
7,37  * * * * ubuntu promjob "id3c etl redcap-det scan-irb-es"       flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-es --commit
9,39  * * * * ubuntu promjob "id3c etl redcap-det scan-irb-zh-Hant"  flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-zh-Hant --commit
11,41 * * * * ubuntu promjob "id3c etl redcap-det scan-irb-kiosk-en" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-kiosk-en --commit
13,43 * * * * ubuntu promjob "id3c etl redcap-det scan-irb-ru"       flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-ru --commit
15,45 * * * * ubuntu promjob "id3c etl redcap-det scan-irb-husky-en" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-husky-en --commit
17,47 * * * * ubuntu promjob "id3c etl redcap-det scan-irb-vi"       flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det scan-irb-vi --commit

# Ingest UW REopening REDCap project every 10 min
GEOCODING_CACHE=/home/ubuntu/uw-reopening-cache.pickle
*/10 * * * * ubuntu promjob "id3c etl redcap-det uw-reopening" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det uw-reopening --redcap-api-batch-size 75 --det-limit 500 --commit

# Ingest Childcare 2021 REDCap project every 30 min (after UW Reopening project above).
# Use redcap-api-batch-size = 1000 because this REDCap project is longitudinal.
GEOCODING_CACHE=/home/ubuntu/sfs-cache.pickle
21,51 * * * * ubuntu promjob "id3c etl redcap-det childcare-2021" flock -F $GEOCODING_CACHE envdir $ENVD/deidentification envdir $ENVD/smartystreets envdir $ENVD/redcap pipenv run id3c etl redcap-det childcare-2021 --redcap-api-batch-size 1000 --commit

# Ingest FHIR documents every 5 minutes.
*/5  * * * * ubuntu fatigue --quiet promjob "id3c etl fhir" pipenv run id3c etl fhir --commit

# Run the idle database session disconnector routine every minute
* * * * * ubuntu fatigue --quiet promjob "terminate-idle-sessions" terminate-idle-sessions
# Run the slow metabase query disconnector routine every minute
* * * * * ubuntu fatigue --quiet promjob "terminate-slow-metabase-queries" terminate-slow-metabase-queries

# Refresh materialized shipping views every 30 minutes to include
# latest data, including latest ingested FHIR documents
*/30 * * * * ubuntu fatigue --quiet promjob "refresh-materialized-shipping-views" flock -F --nonblock --conflict-exit-code 0 "$FLOCK_DIR/refresh_materialized_shipping_views" pipenv run refresh-materialized-shipping-views

# Make UW Reopening testing offers at :15 and :45
# This job uses data created by redcap-det uw-reopening, etl fhir, and materialized shipping views.
25,55  * * * * ubuntu fatigue --quiet promjob "id3c offer-uw-testing" envdir $ENVD/redcap pipenv run id3c offer-uw-testing --commit

# Generate and upload the previous day's linelist data once a day at 6 A.M.
0 6 * * * ubuntu promjob "wa-doh-linelists" chronic pipenv run envdir $ENVD/hutch envdir $ENVD/redcap /opt/backoffice/bin/wa-doh-linelists/generate --date $(date --date=yesterday --iso-8601=date) --output-dir "s3://fh-pi-bedford-t/seattleflu/public-health-reporting"
